{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'knn.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(MODEL_PATH, 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN LIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[19985]: Class CaptureDelegate is implemented in both /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x179d52538) and /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x17b6fc860). One of the two will be used. Which one is undefined.\n",
      "objc[19985]: Class CVWindow is implemented in both /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x179d52588) and /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x177e6ca68). One of the two will be used. Which one is undefined.\n",
      "objc[19985]: Class CVView is implemented in both /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x179d525b0) and /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x177e6ca90). One of the two will be used. Which one is undefined.\n",
      "objc[19985]: Class CVSlider is implemented in both /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x179d525d8) and /Users/prakhar/opt/miniconda3/envs/ML/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x177e6cab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success: continue\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "    pose_landmarks = results.pose_landmarks\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    if pose_landmarks is not None:\n",
    "      pose_landmarks = [[lmk.x, lmk.y, lmk.z] for lmk in pose_landmarks.landmark]\n",
    "      frame_height, frame_width = image.shape[:2]\n",
    "      pose_landmarks *= np.array([frame_width, frame_height, frame_width])\n",
    "      pose_landmarks = np.around(pose_landmarks, 5).flatten()\n",
    "      pose_landmarks = pose_landmarks.reshape(1, -1)\n",
    "\n",
    "      prediction = model.predict(pose_landmarks)\n",
    "\n",
    "      cv2.putText(img=image, text=str(prediction), org=(150, 250), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=3, color=(0, 0, 0),thickness=3)\n",
    "\n",
    "  \n",
    "  \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Pose', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 11:04:07) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08433e0cffb6b31896868c9fb611d1bed4e943d8fd5463e533a0eb267f0d0c65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
